{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "268a96f6",
   "metadata": {},
   "source": [
    "# Understanding the Role of Gender in Book Reviews\n",
    "\n",
    "## CS 6471: Computational Social Science - Project\n",
    "Author: Pratyusha Maiti\n",
    "\n",
    "Affiliation: Georgia Institute of Technology\n",
    "\n",
    "### Research Objectives\n",
    "- Do book reviews by different genders differ in stylistic features?\n",
    "- Do book reviews by different genders differ w.r.t their content?\n",
    "- Are book reviews written by males perceived as more useful than book reviews written by females?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7d023bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pratyushamaiti/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/pratyushamaiti/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/pratyushamaiti/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/pratyushamaiti/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/pratyushamaiti/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /opt/homebrew/anaconda3/lib/python3.8/site-packages (0.1.66)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /opt/homebrew/anaconda3/lib/python3.8/site-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: anyascii in /opt/homebrew/anaconda3/lib/python3.8/site-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n",
      "Requirement already satisfied: pyahocorasick in /opt/homebrew/anaconda3/lib/python3.8/site-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/opt/homebrew/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b38f6f",
   "metadata": {},
   "source": [
    "### Review Data Cleaning\n",
    "\n",
    "Parameters:\n",
    " - Expand contractions\n",
    " - Remove stopwords\n",
    " - Remove junk and html\n",
    " - Tokenize\n",
    " - Lemmatize\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "692d030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6648fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "expand_contractions = True\n",
    "remove_stopwords = False\n",
    "format_text = True\n",
    "tokenize = True\n",
    "lemmatize = False\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Text Preprocessing '''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    split_text = text.split()\n",
    "    \n",
    "    # Expand contractions\n",
    "    if expand_contractions:\n",
    "        expanded_words = []   \n",
    "        for word in split_text:\n",
    "            expanded_words.append(contractions.fix(word))\n",
    "        text = \" \".join(expanded_words)\n",
    "#         print(\"Text after contractions removal: \", text)\n",
    "\n",
    "    # Format words and remove unwanted characters\n",
    "    if format_text:\n",
    "        text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'\\<a href', ' ', text)\n",
    "        text = re.sub(r'&amp;', '', text) \n",
    "        text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "        text = re.sub(r'<br />', ' ', text)\n",
    "        text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # remove stopwords\n",
    "    if remove_stopwords:\n",
    "        split_text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        word = [w for w in split_text if not w in stops]\n",
    "        text = \" \".join(word)\n",
    "#         print(\"Text after stopwords removal: \", text)\n",
    "\n",
    "    # Tokenize each word\n",
    "    if tokenize:\n",
    "        text =  nltk.WordPunctTokenizer().tokenize(text)\n",
    "    \n",
    "    # Lemmatize each token\n",
    "    if lemmatize:\n",
    "        lemm = nltk.stem.WordNetLemmatizer()\n",
    "        text = list(map(lambda word:list(map(lemm.lemmatize, word)), text))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce5af49",
   "metadata": {},
   "source": [
    "##### Generate gendered review data\n",
    "\n",
    "- Genre: Crime, Thriller, Mystery\n",
    "- Joined by userID\n",
    "- Dropped NaN rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6edbd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 852197 entries, 0 to 852196\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   id             852197 non-null  int64  \n",
      " 1   name           850616 non-null  object \n",
      " 2   username       252735 non-null  object \n",
      " 3   age            242043 non-null  float64\n",
      " 4   location       665382 non-null  object \n",
      " 5   joined         820599 non-null  object \n",
      " 6   friends_count  820599 non-null  float64\n",
      " 7   groups_count   820599 non-null  float64\n",
      " 8   reviews_count  820599 non-null  float64\n",
      " 9   first_name     850555 non-null  object \n",
      " 10  gender         852197 non-null  object \n",
      " 11  user_id        852197 non-null  object \n",
      "dtypes: float64(4), int64(1), object(7)\n",
      "memory usage: 78.0+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "gendered_data = pd.read_csv('goodreads_names_id_gender.csv')\n",
    "gendered_data.columns = ['id', 'name', 'username', 'age', 'location', 'joined', 'friends_count','groups_count', 'reviews_count', 'first_name', 'gender', 'user_id']\n",
    "gendered_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb468e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "def load_data(file_name, head = 500):\n",
    "    count = 0\n",
    "    data = []\n",
    "    with gzip.open(file_name) as fin:\n",
    "        for l in fin:\n",
    "            d = json.loads(l)\n",
    "            count += 1\n",
    "            data.append(d)\n",
    "            \n",
    "            # break if reaches the 100th line\n",
    "            if (head is not None) and (count > head):\n",
    "                break\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82ab4d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1849236 entries, 0 to 1849235\n",
      "Data columns (total 11 columns):\n",
      " #   Column        Dtype \n",
      "---  ------        ----- \n",
      " 0   user_id       object\n",
      " 1   book_id       object\n",
      " 2   review_id     object\n",
      " 3   rating        int64 \n",
      " 4   review_text   object\n",
      " 5   date_added    object\n",
      " 6   date_updated  object\n",
      " 7   read_at       object\n",
      " 8   started_at    object\n",
      " 9   n_votes       int64 \n",
      " 10  n_comments    int64 \n",
      "dtypes: int64(3), object(8)\n",
      "memory usage: 155.2+ MB\n"
     ]
    }
   ],
   "source": [
    "goodreads_reviews_mystery_thriller_crime = load_data('goodreads_reviews_mystery_thriller_crime.json.gz',1849235)\n",
    "goodreads_reviews_mystery_thriller_crime_df = pd.DataFrame(goodreads_reviews_mystery_thriller_crime)\n",
    "goodreads_reviews_mystery_thriller_crime_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18363173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1849236 entries, 0 to 1849235\n",
      "Data columns (total 22 columns):\n",
      " #   Column         Dtype  \n",
      "---  ------         -----  \n",
      " 0   user_id        object \n",
      " 1   book_id        object \n",
      " 2   review_id      object \n",
      " 3   rating         int64  \n",
      " 4   review_text    object \n",
      " 5   date_added     object \n",
      " 6   date_updated   object \n",
      " 7   read_at        object \n",
      " 8   started_at     object \n",
      " 9   n_votes        int64  \n",
      " 10  n_comments     int64  \n",
      " 11  id             float64\n",
      " 12  name           object \n",
      " 13  username       object \n",
      " 14  age            float64\n",
      " 15  location       object \n",
      " 16  joined         object \n",
      " 17  friends_count  float64\n",
      " 18  groups_count   float64\n",
      " 19  reviews_count  float64\n",
      " 20  first_name     object \n",
      " 21  gender         object \n",
      "dtypes: float64(5), int64(3), object(14)\n",
      "memory usage: 324.5+ MB\n"
     ]
    }
   ],
   "source": [
    "gendered_reviews_mtc = pd.merge(goodreads_reviews_mystery_thriller_crime_df, gendered_data, on='user_id', how='left')\n",
    "gendered_reviews_mtc.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b81669",
   "metadata": {},
   "source": [
    "Incomplete dataset. Dropped the rows with NaN values in features. We find that this brings out the number of reviews by ~1.5 million reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b983e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 344428 entries, 0 to 344427\n",
      "Data columns (total 22 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   user_id        344428 non-null  object \n",
      " 1   book_id        344428 non-null  object \n",
      " 2   review_id      344428 non-null  object \n",
      " 3   rating         344428 non-null  int64  \n",
      " 4   review_text    344428 non-null  object \n",
      " 5   date_added     344428 non-null  object \n",
      " 6   date_updated   344428 non-null  object \n",
      " 7   read_at        344428 non-null  object \n",
      " 8   started_at     344428 non-null  object \n",
      " 9   n_votes        344428 non-null  int64  \n",
      " 10  n_comments     344428 non-null  int64  \n",
      " 11  id             344428 non-null  float64\n",
      " 12  name           344428 non-null  object \n",
      " 13  username       344428 non-null  object \n",
      " 14  age            344428 non-null  float64\n",
      " 15  location       344428 non-null  object \n",
      " 16  joined         344428 non-null  object \n",
      " 17  friends_count  344428 non-null  float64\n",
      " 18  groups_count   344428 non-null  float64\n",
      " 19  reviews_count  344428 non-null  float64\n",
      " 20  first_name     344428 non-null  object \n",
      " 21  gender         344428 non-null  object \n",
      "dtypes: float64(5), int64(3), object(14)\n",
      "memory usage: 57.8+ MB\n"
     ]
    }
   ],
   "source": [
    "clean_gendered_reviews_mtc = gendered_reviews_mtc.dropna()\n",
    "clean_gendered_reviews_mtc = clean_gendered_reviews_mtc.reset_index(drop=True)\n",
    "clean_gendered_reviews_mtc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1055c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "female    177160\n",
       "male       72531\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_gendered_reviews_mtc['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e591658e",
   "metadata": {},
   "source": [
    "Dropped the rows with *unknown gender type*. We find that ratio of female to male reviewers in the test dataset is ~ 2.5:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab5fdc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 249691 entries, 0 to 249690\n",
      "Data columns (total 22 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   user_id        249691 non-null  object \n",
      " 1   book_id        249691 non-null  object \n",
      " 2   review_id      249691 non-null  object \n",
      " 3   rating         249691 non-null  int64  \n",
      " 4   review_text    249691 non-null  object \n",
      " 5   date_added     249691 non-null  object \n",
      " 6   date_updated   249691 non-null  object \n",
      " 7   read_at        249691 non-null  object \n",
      " 8   started_at     249691 non-null  object \n",
      " 9   n_votes        249691 non-null  int64  \n",
      " 10  n_comments     249691 non-null  int64  \n",
      " 11  id             249691 non-null  float64\n",
      " 12  name           249691 non-null  object \n",
      " 13  username       249691 non-null  object \n",
      " 14  age            249691 non-null  float64\n",
      " 15  location       249691 non-null  object \n",
      " 16  joined         249691 non-null  object \n",
      " 17  friends_count  249691 non-null  float64\n",
      " 18  groups_count   249691 non-null  float64\n",
      " 19  reviews_count  249691 non-null  float64\n",
      " 20  first_name     249691 non-null  object \n",
      " 21  gender         249691 non-null  object \n",
      "dtypes: float64(5), int64(3), object(14)\n",
      "memory usage: 41.9+ MB\n"
     ]
    }
   ],
   "source": [
    "clean_gendered_reviews_mtc = clean_gendered_reviews_mtc[clean_gendered_reviews_mtc.gender != 'unknown']\n",
    "clean_gendered_reviews_mtc = clean_gendered_reviews_mtc.reset_index(drop=True)\n",
    "clean_gendered_reviews_mtc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "006cb9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Fun to head back to #1 after reading 3 of the more recent (one more new one yet to read). And interesting fact: I'd always assumed references to prior cases were references to prior books. However, here in the first book are references to prior cases. Either it is just part of the needed context or there will be a new novel set before this first one. Time will tell. \\n Also amusing. Here in the first one, cell phones worked in Three pines. In the later ones they don't. \\n And as far as I can tell, an inconsistency within this first one: an early scene takes place in the bistro and it is about a delay; later when that delay is explained, it also happened at the bistro, so would have been impossible for the waiting person to have missed it. Small detail. \\n One thing I enjoy about Penny'e mysteries is the characterization, and it is present in this early book. And she does well at red herring clues among the real clues.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gendered_reviews_mtc['review_text'][323]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae96e226",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "- Generate clean review data on the reduced dataset\n",
    "- Generate writing style features\n",
    "- Generate content features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7c3597a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an',\n",
       " 'amazing',\n",
       " 'and',\n",
       " 'unique',\n",
       " 'creation',\n",
       " 'jj',\n",
       " 'abrams',\n",
       " 'and',\n",
       " 'doug',\n",
       " 'dorst',\n",
       " 'created',\n",
       " 'what',\n",
       " 'reads',\n",
       " 'like',\n",
       " 'a',\n",
       " 'classic',\n",
       " 'work',\n",
       " 'of',\n",
       " 'fiction',\n",
       " 'something',\n",
       " 'you',\n",
       " 'can',\n",
       " 'easily',\n",
       " 'imagine',\n",
       " 'having',\n",
       " 'read',\n",
       " 'in',\n",
       " 'english',\n",
       " 'class',\n",
       " 'and',\n",
       " 'then',\n",
       " 'wrote',\n",
       " 'a',\n",
       " 'intriguing',\n",
       " 'side',\n",
       " 'story',\n",
       " 'in',\n",
       " 'the',\n",
       " 'margins',\n",
       " 'a',\n",
       " 'grad',\n",
       " 'student',\n",
       " 'eric',\n",
       " 'has',\n",
       " 'left',\n",
       " 'his',\n",
       " 'annotated',\n",
       " 'copy',\n",
       " 'of',\n",
       " 'sot',\n",
       " 'ship',\n",
       " 'of',\n",
       " 'theseus',\n",
       " 'in',\n",
       " 'the',\n",
       " 'library',\n",
       " 'and',\n",
       " 'an',\n",
       " 'undergrad',\n",
       " 'jen',\n",
       " 'finds',\n",
       " 'it',\n",
       " 'and',\n",
       " 'replies',\n",
       " 'to',\n",
       " 'his',\n",
       " 'annotations',\n",
       " 'this',\n",
       " 'leads',\n",
       " 'to',\n",
       " 'them',\n",
       " 'making',\n",
       " 'exciting',\n",
       " 'discoveries',\n",
       " 'about',\n",
       " 'the',\n",
       " 'book',\n",
       " 'and',\n",
       " 'also',\n",
       " 'falling',\n",
       " 'in',\n",
       " 'love',\n",
       " 'the',\n",
       " 'first',\n",
       " 'thing',\n",
       " 'is',\n",
       " 'this',\n",
       " 'book',\n",
       " 'is',\n",
       " 'just',\n",
       " 'beautifully',\n",
       " 'printed',\n",
       " 'it',\n",
       " 'looks',\n",
       " 'and',\n",
       " 'feels',\n",
       " 'like',\n",
       " 'a',\n",
       " 'classic',\n",
       " 'book',\n",
       " 'to',\n",
       " 'the',\n",
       " 'point',\n",
       " 'where',\n",
       " 'people',\n",
       " 'would',\n",
       " 'ask',\n",
       " 'me',\n",
       " 'why',\n",
       " 'i',\n",
       " 'am',\n",
       " 'reading',\n",
       " 'such',\n",
       " 'an',\n",
       " 'old',\n",
       " 'book',\n",
       " 'the',\n",
       " 'marginalia',\n",
       " 'feels',\n",
       " 'real',\n",
       " 'and',\n",
       " 'i',\n",
       " 'can',\n",
       " 'only',\n",
       " 'imagine',\n",
       " 'how',\n",
       " 'hard',\n",
       " 'that',\n",
       " 'was',\n",
       " 'to',\n",
       " 'print',\n",
       " 'this',\n",
       " 'is',\n",
       " 'one',\n",
       " 'that',\n",
       " 'will',\n",
       " 'be',\n",
       " 'cool',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'on',\n",
       " 'the',\n",
       " 'shelves',\n",
       " 'only',\n",
       " 'complaint',\n",
       " 'is',\n",
       " 'the',\n",
       " 'inserts',\n",
       " 'are',\n",
       " 'a',\n",
       " 'cool',\n",
       " 'idea',\n",
       " 'but',\n",
       " 'they',\n",
       " 'fall',\n",
       " 'out',\n",
       " 'all',\n",
       " 'the',\n",
       " 'time',\n",
       " 'and',\n",
       " 'at',\n",
       " 'this',\n",
       " 'point',\n",
       " 'i',\n",
       " 'have',\n",
       " 'no',\n",
       " 'idea',\n",
       " 'which',\n",
       " 'pages',\n",
       " 'they',\n",
       " 'were',\n",
       " 'supposed',\n",
       " 'to',\n",
       " 'be',\n",
       " 'on',\n",
       " 'the',\n",
       " 'most',\n",
       " 'interesting',\n",
       " 'thing',\n",
       " 'about',\n",
       " 'this',\n",
       " 'book',\n",
       " 'to',\n",
       " 'me',\n",
       " 'is',\n",
       " 'that',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'blueprint',\n",
       " 'for',\n",
       " 'how',\n",
       " 'people',\n",
       " 'discuss',\n",
       " 'books',\n",
       " 'sometimes',\n",
       " 'they',\n",
       " 'discuss',\n",
       " 'the',\n",
       " 'content',\n",
       " 'of',\n",
       " 'the',\n",
       " 'book',\n",
       " 'other',\n",
       " 'times',\n",
       " 'they',\n",
       " 'use',\n",
       " 'the',\n",
       " 'content',\n",
       " 'as',\n",
       " 'a',\n",
       " 'jumping',\n",
       " 'off',\n",
       " 'point',\n",
       " 'to',\n",
       " 'have',\n",
       " 'a',\n",
       " 'personal',\n",
       " 'discussion',\n",
       " 'the',\n",
       " 'nature',\n",
       " 'of',\n",
       " 'this',\n",
       " 'book',\n",
       " 'shows',\n",
       " 'both',\n",
       " 'almost',\n",
       " 'split',\n",
       " '50',\n",
       " '50',\n",
       " 'a',\n",
       " 'key',\n",
       " 'difference',\n",
       " 'with',\n",
       " 'this',\n",
       " 'book',\n",
       " 'and',\n",
       " 'the',\n",
       " 'genius',\n",
       " 'of',\n",
       " 'it',\n",
       " 'is',\n",
       " 'that',\n",
       " 'most',\n",
       " 'of',\n",
       " 'us',\n",
       " 'have',\n",
       " 'to',\n",
       " 'rely',\n",
       " 'on',\n",
       " 'memory',\n",
       " 'to',\n",
       " 'start',\n",
       " 'a',\n",
       " 'conversation',\n",
       " 'with',\n",
       " 'someone',\n",
       " 'about',\n",
       " 'a',\n",
       " 'particular',\n",
       " 'event',\n",
       " 'or',\n",
       " 'passage',\n",
       " 'in',\n",
       " 'the',\n",
       " 'book',\n",
       " 'here',\n",
       " 'our',\n",
       " 'characters',\n",
       " 'can',\n",
       " 'literally',\n",
       " 'underline',\n",
       " 'a',\n",
       " 'phrase',\n",
       " 'like',\n",
       " 'relationships',\n",
       " 'and',\n",
       " 'then',\n",
       " 'talk',\n",
       " 'about',\n",
       " 'what',\n",
       " 'is',\n",
       " 'happening',\n",
       " 'in',\n",
       " 'their',\n",
       " 'relationship',\n",
       " 'at',\n",
       " 'the',\n",
       " 'moment',\n",
       " 'one',\n",
       " 'thing',\n",
       " 'i',\n",
       " 'did',\n",
       " 'not',\n",
       " 'realize',\n",
       " 'until',\n",
       " 'later',\n",
       " 'in',\n",
       " 'the',\n",
       " 'book',\n",
       " 'is',\n",
       " 'that',\n",
       " 'the',\n",
       " 'ship',\n",
       " 'of',\n",
       " 'theseus',\n",
       " 'is',\n",
       " 'a',\n",
       " 'real',\n",
       " 'thing',\n",
       " 'it',\n",
       " 'is',\n",
       " 'an',\n",
       " 'ancient',\n",
       " 'greek',\n",
       " 'philosophical',\n",
       " 'question',\n",
       " 'if',\n",
       " 'you',\n",
       " 'replace',\n",
       " 'all',\n",
       " 'the',\n",
       " 'parts',\n",
       " 'of',\n",
       " 'something',\n",
       " 'is',\n",
       " 'it',\n",
       " 'still',\n",
       " 'the',\n",
       " 'same',\n",
       " 'object',\n",
       " 'this',\n",
       " 'inspires',\n",
       " 'the',\n",
       " 'ship',\n",
       " 'that',\n",
       " 's',\n",
       " 'is',\n",
       " 'captured',\n",
       " 'on',\n",
       " 'which',\n",
       " 'sails',\n",
       " 'through',\n",
       " 'the',\n",
       " 'mists',\n",
       " 'of',\n",
       " 'time',\n",
       " 'always',\n",
       " 'getting',\n",
       " 'fixed',\n",
       " 'when',\n",
       " 'needed',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'ship',\n",
       " 'that',\n",
       " 'cannot',\n",
       " 'die',\n",
       " 'or',\n",
       " 'a',\n",
       " 'series',\n",
       " 'of',\n",
       " 'ships',\n",
       " 'i',\n",
       " 'found',\n",
       " 'the',\n",
       " 'story',\n",
       " 'of',\n",
       " 'jen',\n",
       " 'eric',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'more',\n",
       " 'compelling',\n",
       " 'than',\n",
       " 'the',\n",
       " 'sot',\n",
       " 'story',\n",
       " 'they',\n",
       " 'were',\n",
       " 'real',\n",
       " 'characters',\n",
       " 'that',\n",
       " 'you',\n",
       " 'could',\n",
       " 'relate',\n",
       " 'to',\n",
       " 'sot',\n",
       " 'felt',\n",
       " 'like',\n",
       " 'books',\n",
       " 'you',\n",
       " 'read',\n",
       " 'in',\n",
       " 'english',\n",
       " 'class',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'obtuse',\n",
       " 'and',\n",
       " 'i',\n",
       " 'still',\n",
       " 'do',\n",
       " 'not',\n",
       " 'understand',\n",
       " 'it',\n",
       " 'all',\n",
       " 'but',\n",
       " 'do',\n",
       " 'not',\n",
       " 'get',\n",
       " 'me',\n",
       " 'wrong',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'pretty',\n",
       " 'cool',\n",
       " 'story',\n",
       " 'i',\n",
       " 'particularly',\n",
       " 'enjoyed',\n",
       " 'the',\n",
       " 'themes',\n",
       " 'like',\n",
       " 'how',\n",
       " 'it',\n",
       " 'all',\n",
       " 'begins',\n",
       " 'and',\n",
       " 'ends',\n",
       " 'with',\n",
       " 'water',\n",
       " 'and',\n",
       " 'also',\n",
       " 'how',\n",
       " 'it',\n",
       " 'was',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'about',\n",
       " 'straka',\n",
       " 's',\n",
       " 'regret',\n",
       " 'to',\n",
       " 'fulfill',\n",
       " 'his',\n",
       " 'mission',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'living',\n",
       " 'his',\n",
       " 'life',\n",
       " 'with',\n",
       " 'his',\n",
       " 'true',\n",
       " 'love',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'like',\n",
       " 'how',\n",
       " 's',\n",
       " 'was',\n",
       " 'on',\n",
       " 'a',\n",
       " 'mission',\n",
       " 'but',\n",
       " 'did',\n",
       " 'not',\n",
       " 'really',\n",
       " 'know',\n",
       " 'why',\n",
       " 'he',\n",
       " 'just',\n",
       " 'found',\n",
       " 'himself',\n",
       " 'caught',\n",
       " 'up',\n",
       " 'in',\n",
       " 'it']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(clean_gendered_reviews_mtc['review_text'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7dd33003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /opt/homebrew/anaconda3/lib/python3.8/site-packages (4.59.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/opt/homebrew/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6591f740",
   "metadata": {},
   "outputs": [],
   "source": [
    "gendered_reviews_mtc['review_text'][0] = ' '.join(clean_text(gendered_reviews_mtc['review_text'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38f0af49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249691/249691 [15:16<00:00, 272.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "for idx in tqdm(range(clean_gendered_reviews_mtc.shape[0])):\n",
    "    clean_gendered_reviews_mtc['review_text'][idx] = ' '.join(clean_text(clean_gendered_reviews_mtc['review_text'][idx]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03344c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 249691 entries, 0 to 249690\n",
      "Data columns (total 22 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   user_id        249691 non-null  object \n",
      " 1   book_id        249691 non-null  object \n",
      " 2   review_id      249691 non-null  object \n",
      " 3   rating         249691 non-null  int64  \n",
      " 4   review_text    249691 non-null  object \n",
      " 5   date_added     249691 non-null  object \n",
      " 6   date_updated   249691 non-null  object \n",
      " 7   read_at        249691 non-null  object \n",
      " 8   started_at     249691 non-null  object \n",
      " 9   n_votes        249691 non-null  int64  \n",
      " 10  n_comments     249691 non-null  int64  \n",
      " 11  id             249691 non-null  float64\n",
      " 12  name           249691 non-null  object \n",
      " 13  username       249691 non-null  object \n",
      " 14  age            249691 non-null  float64\n",
      " 15  location       249691 non-null  object \n",
      " 16  joined         249691 non-null  object \n",
      " 17  friends_count  249691 non-null  float64\n",
      " 18  groups_count   249691 non-null  float64\n",
      " 19  reviews_count  249691 non-null  float64\n",
      " 20  first_name     249691 non-null  object \n",
      " 21  gender         249691 non-null  object \n",
      "dtypes: float64(5), int64(3), object(14)\n",
      "memory usage: 41.9+ MB\n"
     ]
    }
   ],
   "source": [
    "clean_gendered_reviews_mtc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc09c56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_gendered_reviews_mtc = clean_gendered_reviews_mtc[len(clean_gendered_reviews_mtc.review_text)>0]\n",
    "# clean_gendered_reviews_mtc = clean_gendered_reviews_mtc.reset_index(drop=True)\n",
    "# clean_gendered_reviews_mtc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c644209",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"cleaned_gendered_reviews_mtc\"\n",
    "clean_gendered_reviews_mtc.to_csv(file_name, sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe4cd32",
   "metadata": {},
   "source": [
    "### Writing Style Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ebcf7a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features under analysis:\n",
    "lexical_markers = []\n",
    "vocabulary_richness = []\n",
    "complexity = []\n",
    "hedging = []\n",
    "use_of_pronouns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c10e9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## small set of reviews taken into consideration\n",
    "\n",
    "small_gendered_reviews_mtc = clean_gendered_reviews_mtc\n",
    "\n",
    "# small_gendered_reviews_mtc = clean_gendered_reviews_mtc[:40000]\n",
    "# small_gendered_reviews_mtc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f24f92db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 16), ('and', 14), ('is', 11), ('a', 11), ('to', 10)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topN_Freq(small_gendered_reviews_mtc['review_text'][44].split(), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d5cff7",
   "metadata": {},
   "source": [
    "#### Get Top 50 common words in the entire reviewset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aaf560da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249691/249691 [00:21<00:00, 11625.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "def topN_Freq(data, n: int):\n",
    "    fdist = FreqDist(data)\n",
    "    return fdist.most_common(n)\n",
    "\n",
    "top_50 = {}\n",
    "dic2 = {}\n",
    "for idx in tqdm(range(small_gendered_reviews_mtc.shape[0])):\n",
    "    \n",
    "    freqDict = topN_Freq(small_gendered_reviews_mtc['review_text'][idx].split(), 50)\n",
    "    dic1 = dict(freqDict)\n",
    "    if idx != 0:\n",
    "        result = {k: dic1.get(k, 0) + dic2.get(k, 0) for k in set(dic1) | set(dic2)}\n",
    "    dic2 = dict(Counter(top_50).most_common(50))\n",
    "\n",
    "top_50 = dict(Counter(top_50).most_common(50))\n",
    "len(top_50)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7418ac6b",
   "metadata": {},
   "source": [
    "#### Get all occurances of the top 50 frequent words in each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49743b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249691/249691 [00:06<00:00, 40114.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "249691"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical_markers = []\n",
    "for idx in tqdm(range(small_gendered_reviews_mtc.shape[0])):\n",
    "    freq = Counter(small_gendered_reviews_mtc['review_text'][idx].split())\n",
    "    lexical_markers.append(sum(freq.get(k, 0) for k in set(result)))\n",
    "len(lexical_markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd976cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "def avgSentLenghtByWord(text):\n",
    "    tokens = sent_tokenize(text)\n",
    "    return np.average([len(token.split()) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e575e6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countFunctionalWords(text):\n",
    "    functional_words = \"\"\"a between in nor some upon\n",
    "    about both including nothing somebody us\n",
    "    above but inside of someone used\n",
    "    after by into off something via\n",
    "    all can is on such we\n",
    "    although cos it once than what\n",
    "    am do its one that whatever\n",
    "    among down latter onto the when\n",
    "    an each less opposite their where\n",
    "    and either like or them whether\n",
    "    another enough little our these which\n",
    "    any every lots outside they while\n",
    "    anybody everybody many over this who\n",
    "    anyone everyone me own those whoever\n",
    "    anything everything more past though whom\n",
    "    are few most per through whose\n",
    "    around following much plenty till will\n",
    "    as for must plus to with\n",
    "    at from my regarding toward within\n",
    "    be have near same towards without\n",
    "    because he need several under worth\n",
    "    before her neither she unless would\n",
    "    behind him no should unlike yes\n",
    "    below i nobody since until you\n",
    "    beside if none so up your\n",
    "    \"\"\"\n",
    "\n",
    "    functional_words = functional_words.split()\n",
    "    count = 0\n",
    "\n",
    "    for i in text:\n",
    "        if i in functional_words:\n",
    "            count += 1\n",
    "\n",
    "    return count / len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "401383b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249691/249691 [00:05<00:00, 49124.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hedgeWords = pd.read_csv(\"hedgeWords.txt\", sep=\"\\n\")\n",
    "hedgefile = open(\"hedgeWords.txt\", \"r\")\n",
    "content = hedgefile.read()\n",
    "hedgeWords = content.split(\"\\n\")\n",
    "hedgefile.close()\n",
    "\n",
    "hedging = []\n",
    "for idx in tqdm(range(small_gendered_reviews_mtc.shape[0])):\n",
    "    hedging.append(sum(x == y for x, y in zip(small_gendered_reviews_mtc['review_text'], hedgeWords)))\n",
    "len(hedging[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5d34535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249691/249691 [00:09<00:00, 27683.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "249691"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "def yulesCharacteristicK(words):\n",
    "    N = len(words)\n",
    "    K=0\n",
    "    if N>0:\n",
    "        freqs = Counter()\n",
    "        freqs.update(words)\n",
    "        vi = Counter()\n",
    "        vi.update(freqs.values())\n",
    "        M = sum([(value * value) * vi[value] for key, value in freqs.items()])\n",
    "        K = 10000 * (M - N) / math.pow(N, 2)\n",
    "    return K\n",
    "vocabulary_richness = []\n",
    "for idx in tqdm(range(small_gendered_reviews_mtc.shape[0])):\n",
    "    vocabulary_richness.append(yulesCharacteristicK(small_gendered_reviews_mtc['review_text'][idx]))\n",
    "len(vocabulary_richness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "935ad383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249691/249691 [00:53<00:00, 4687.68it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(249691, 249691, 249691)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def avg_wordLength(text):\n",
    "    return np.average([len(word) for word in text.split()])\n",
    "\n",
    "def avg_SentLenghtByCh(text):\n",
    "    tokens = sent_tokenize(text)\n",
    "    return np.average([len(token) for token in tokens])\n",
    "\n",
    "def avg_SentLenghtByWord(text):\n",
    "    tokens = sent_tokenize(text)\n",
    "    return np.average([len(token.split()) for token in tokens])\n",
    "\n",
    "complexity_1 = []\n",
    "complexity_2 = []\n",
    "complexity_3 = []\n",
    "for idx in tqdm(range(small_gendered_reviews_mtc.shape[0])):\n",
    "    if len(small_gendered_reviews_mtc['review_text'][idx])>0:\n",
    "        complexity_1.append(avg_wordLength(small_gendered_reviews_mtc['review_text'][idx]))\n",
    "        complexity_2.append(avg_SentLenghtByCh(small_gendered_reviews_mtc['review_text'][idx]))\n",
    "        complexity_3.append(avg_SentLenghtByWord(small_gendered_reviews_mtc['review_text'][idx]))\n",
    "#         complexity.append([avg_wordLength(small_gendered_reviews_mtc['review_text'][idx]), avg_SentLenghtByCh(small_gendered_reviews_mtc['review_text'][idx]), avg_SentLenghtByWord(small_gendered_reviews_mtc['review_text'][idx])])\n",
    "    else:\n",
    "        complexity_1.append(0)\n",
    "        complexity_2.append(0)\n",
    "        complexity_3.append(0)\n",
    "len(complexity_1), len(complexity_2), len(complexity_3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "efab1706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import word_tokenize, pos_tag\n",
    "\n",
    "# pronouns = []\n",
    "# for idx in tqdm(range(small_gendered_reviews_mtc.shape[0])):\n",
    "#     if len(small_gendered_reviews_mtc['review_text'][idx])>0:\n",
    "#         pronouns.append(sum(1 for word, pos in pos_tag(word_tokenize(small_gendered_reviews_mtc['review_text'][idx]), tagset='universal') if pos =='PRON')/len(word_tokenize(small_gendered_reviews_mtc['review_text'][idx])))\n",
    "#     else:\n",
    "#         pronouns.append(0)\n",
    "    \n",
    "# len(pronouns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a04e3d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249691/249691 [22:50<00:00, 182.21it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "249691"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "pos_pronouns = []\n",
    "for idx in tqdm(range(small_gendered_reviews_mtc.shape[0])):\n",
    "    if len(small_gendered_reviews_mtc['review_text'][idx])>0:\n",
    "        pos_pronouns.append(sum(1 for word, pos in pos_tag(word_tokenize(small_gendered_reviews_mtc['review_text'][idx]), tagset='universal') if pos.startswith('PR'))/len(word_tokenize(small_gendered_reviews_mtc['review_text'][idx])))\n",
    "    else:\n",
    "        pos_pronouns.append(0)\n",
    "    \n",
    "len(pos_pronouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99dea57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11a792a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a650766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaaf01f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdca5a43",
   "metadata": {},
   "source": [
    "### Testing out Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "385ed78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df754c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fbcb5b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 249691 entries, 0 to 249690\n",
      "Data columns (total 6 columns):\n",
      " #   Column               Non-Null Count   Dtype  \n",
      "---  ------               --------------   -----  \n",
      " 0   frequency            249691 non-null  int64  \n",
      " 1   vocabulary_richness  249691 non-null  float64\n",
      " 2   pronouns             249691 non-null  float64\n",
      " 3   complexity1          249691 non-null  float64\n",
      " 4   complexity2          249691 non-null  float64\n",
      " 5   complexity3          249691 non-null  float64\n",
      "dtypes: float64(5), int64(1)\n",
      "memory usage: 11.4 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d15f4521",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['frequency'] = lexical_markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d622b4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['hedging'] = hedging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "226a2a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vocabulary_richness'] = vocabulary_richness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e6ceabff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['complexity1'] = complexity_1\n",
    "df['complexity2'] = complexity_2\n",
    "df['complexity3'] = complexity_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c7a3f84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pronouns'] = pos_pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "de183923",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, testing_data = train_test_split(small_gendered_reviews_mtc,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6910cb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 187268 entries, 198069 to 199340\n",
      "Data columns (total 22 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   user_id        187268 non-null  object \n",
      " 1   book_id        187268 non-null  object \n",
      " 2   review_id      187268 non-null  object \n",
      " 3   rating         187268 non-null  int64  \n",
      " 4   review_text    187268 non-null  object \n",
      " 5   date_added     187268 non-null  object \n",
      " 6   date_updated   187268 non-null  object \n",
      " 7   read_at        187268 non-null  object \n",
      " 8   started_at     187268 non-null  object \n",
      " 9   n_votes        187268 non-null  int64  \n",
      " 10  n_comments     187268 non-null  int64  \n",
      " 11  id             187268 non-null  float64\n",
      " 12  name           187268 non-null  object \n",
      " 13  username       187268 non-null  object \n",
      " 14  age            187268 non-null  float64\n",
      " 15  location       187268 non-null  object \n",
      " 16  joined         187268 non-null  object \n",
      " 17  friends_count  187268 non-null  float64\n",
      " 18  groups_count   187268 non-null  float64\n",
      " 19  reviews_count  187268 non-null  float64\n",
      " 20  first_name     187268 non-null  object \n",
      " 21  gender         187268 non-null  object \n",
      "dtypes: float64(5), int64(3), object(14)\n",
      "memory usage: 32.9+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 62423 entries, 211237 to 107233\n",
      "Data columns (total 22 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   user_id        62423 non-null  object \n",
      " 1   book_id        62423 non-null  object \n",
      " 2   review_id      62423 non-null  object \n",
      " 3   rating         62423 non-null  int64  \n",
      " 4   review_text    62423 non-null  object \n",
      " 5   date_added     62423 non-null  object \n",
      " 6   date_updated   62423 non-null  object \n",
      " 7   read_at        62423 non-null  object \n",
      " 8   started_at     62423 non-null  object \n",
      " 9   n_votes        62423 non-null  int64  \n",
      " 10  n_comments     62423 non-null  int64  \n",
      " 11  id             62423 non-null  float64\n",
      " 12  name           62423 non-null  object \n",
      " 13  username       62423 non-null  object \n",
      " 14  age            62423 non-null  float64\n",
      " 15  location       62423 non-null  object \n",
      " 16  joined         62423 non-null  object \n",
      " 17  friends_count  62423 non-null  float64\n",
      " 18  groups_count   62423 non-null  float64\n",
      " 19  reviews_count  62423 non-null  float64\n",
      " 20  first_name     62423 non-null  object \n",
      " 21  gender         62423 non-null  object \n",
      "dtypes: float64(5), int64(3), object(14)\n",
      "memory usage: 11.0+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.info(), testing_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3c6b1e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=training_data['gender'].values\n",
    "Y_test=testing_data['gender'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "63ab1813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df,field, training_data,testing_data):\n",
    "    \"\"\"Extract features using different methods\"\"\" \n",
    "        \n",
    "    # TF-IDF BASED FEATURE REPRESENTATION\n",
    "    tfidf_vectorizer=TfidfVectorizer(use_idf=True, max_df=0.95)\n",
    "    tfidf_vectorizer.fit_transform(training_data[field].values)\n",
    "\n",
    "    train_feature_set=tfidf_vectorizer.transform(training_data[field].values)\n",
    "    test_feature_set=tfidf_vectorizer.transform(testing_data[field].values)\n",
    "\n",
    "    return train_feature_set,test_feature_set,tfidf_vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "37233fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,feature_transformer=extract_features(small_gendered_reviews_mtc, 'review_text', training_data,testing_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "95a882e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=small_gendered_reviews_mtc[:200000]['gender'].values\n",
    "Y_test=small_gendered_reviews_mtc[200000:]['gender'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "20d7dcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    }
   ],
   "source": [
    "scikit_log_reg = LogisticRegression(verbose=1, solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=10000)\n",
    "model=scikit_log_reg.fit(df[:200000],Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e14f4d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['female', 'female', 'female', ..., 'female', 'female', 'female'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = model.predict(df[200000:])\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "852f90ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.72681076, 0.27318924],\n",
       "       [0.77564161, 0.22435839],\n",
       "       [0.71933223, 0.28066777],\n",
       "       ...,\n",
       "       [0.70166609, 0.29833391],\n",
       "       [0.70795074, 0.29204926],\n",
       "       [0.7862406 , 0.2137594 ]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = model.predict_proba(df[200000:])\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "acece29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  0.6819947274154273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "\n",
    "print((\"Accuracy  \") + str(accuracy_score(Y_test, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "58ca951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_n = [0 if y == \"male\" else 1 for y in Y_test]\n",
    "probs_n = [i[0] for i in probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bef4d385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance:  -0.0045\n",
      "mean_squared_log_error:  0.108\n",
      "r2:  -0.0083\n",
      "MAE:  0.4197\n",
      "MSE:  0.2175\n",
      "RMSE:  0.4664\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "def regression_results(y_true, y_pred):\n",
    "\n",
    "    # Regression metrics\n",
    "    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n",
    "    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n",
    "    mse=metrics.mean_squared_error(y_true, y_pred) \n",
    "    mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\n",
    "    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n",
    "    r2=metrics.r2_score(y_true, y_pred)\n",
    "\n",
    "    print('explained_variance: ', round(explained_variance,4))    \n",
    "    print('mean_squared_log_error: ', round(mean_squared_log_error,4))\n",
    "    print('r2: ', round(r2,4))\n",
    "    print('MAE: ', round(mean_absolute_error,4))\n",
    "    print('MSE: ', round(mse,4))\n",
    "    print('RMSE: ', round(np.sqrt(mse),4))\n",
    "    \n",
    "regression_results(Y_test_n, probs_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf7f021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7547a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79de7a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca82137c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56508d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_predictions(model,X_test,k):\n",
    "    \n",
    "    # get probabilities instead of predicted labels, since we want to collect top 3\n",
    "    probs = model.predict_proba(X_test)\n",
    "    # GET TOP K PREDICTIONS BY PROB - note these are just index\n",
    "    best_n = np.argsort(probs, axis=1)[:,-k:]\n",
    "\n",
    "    # GET CATEGORY OF PREDICTIONS\n",
    "    preds=[[model.classes_[predicted_cat] for predicted_cat in prediction] for prediction in best_n]\n",
    "\n",
    "    # REVERSE CATEGORIES - DESCENDING ORDER OF IMPORTANCE\n",
    "    preds=[ item[::-1] for item in preds]\n",
    "\n",
    "    return preds\n",
    "\n",
    "probs = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24763fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81be979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90c189b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58de0dcb",
   "metadata": {},
   "source": [
    "### Content Features\n",
    "\n",
    "Perplexity, OOV words, Entropy and other features. Will be explained later :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "da5c9275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sc\n",
    "\n",
    "def ShannonEntropy(text):\n",
    "    freqs = Counter()\n",
    "    freqs.update(words)\n",
    "    arr = np.array(list(freqs.values()))\n",
    "    distribution = 1. * arr\n",
    "    distribution /= max(1, len(words))\n",
    "    \n",
    "    H = sc.stats.entropy(distribution, base=2)\n",
    "    return H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0bcdca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimpsonsIndex(text):\n",
    "    freqs = Counter()\n",
    "    freqs.update(words)\n",
    "    N = len(words)\n",
    "    n = sum([1.0 * i * (i - 1) for i in freqs.values()])\n",
    "    D = 1 - (n / (N * (N - 1)))\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722c6896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4cf622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1096c877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e119c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d860485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883ba899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4785bfc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14be20da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab954a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37c2822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f45f28a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
